<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Akash Kumar</title>
  
  <meta name="author" content="Akash Kumar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Akash Kumar</name>
              </p>
              <!-- <p style="font-size:15px"> -->
              <p> 
                I am a fifth year PhD student at Center for Research in Computer Vision (CRCV), University of Central Florida (UCF), under the supervision of <a href="https://www.crcv.ucf.edu/person/rawat/" style="font-size:15px">Prof. Yogesh Singh Rawat</a>. 
              </p>
              <p style="font-size:15px">
                I have a broad interest in deep learning and computer vision. My current research mainly focuses on <strong>data-efficient</strong> approaches for multimodal and unimodal <strong>dense video tasks</strong>.
              </p>
              <p style="color:red;"><b> Looking for research intern/full-time positions (Summer'25)! Feel free to drop me an email. </b></p>
              <p style="text-align:center">
                <a href="mailto:akash.kumar@ucf.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=gsHhV5kAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/AKASH2907">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/akash-kumar-498600113/">LinkedIn</a> &nbsp/&nbsp
                <a href="data/AkashKumarCV.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_pic.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Intern Experience</heading>
            </td>
          </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/amazon_logo.png' width="110">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Applied Scientist Intern</strong>
                <br> Visual Shopping Team, Palo Alto, USA. Summer 2024
                <br> Host: <a href="https://shanyang.me/">Shan Yang</a>,
                <a href="https://www.amazon.science/author/junbang-liang">Junbang Liang</a>, 
                <a href="https://www.sampathchanda.com/">Sampath Chanda.</a>
                <!-- <a href="https://www.amazon.science/author/yusheng-xie">Yusheng Xie</a>,
                <a href="https://scholar.google.com/citations?user=Z_WrhK8AAAAJ&hl=en">Mu Li</a> -->
                <p></p>
                <p> Towards Open-vocabulary video object understanding.</p>
              </td>
            </tr>
          </tr>
          </table>

          <!-- <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/bytedance.jpg' width="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Intern</strong>
                <br> ByteDance Inc., Mountain View, USA. Summer 2021
                <br> Host: <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>,
                <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a>
                <p></p>
                <p>Efficient neural architecture search.</p>
              </td>
            </tr>
          </tr>
          </table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            
            <p>Below is a selected list of my works (in <strong>chronological order</strong>), representative papers are <span style="background-color: #ffffd0;">highlighted</span>.</p>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stpro_teaser.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2412.07072"> -->
                <papertitle>STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</papertitle>
              </a>
              <br>
              Aaryan Garg, <strong> Akash Kumar, </strong>
              Yogesh Singh Rawat
              <br>
              <em>Under review </em>
              <br>
              <!-- <a href="https://arxiv.org/abs/2412.07072">paper</a> &nbsp/&nbsp -->
              <!-- <a href="https://github.com/AKASH2907/stable-mean-teacher">code</a> -->
              <p></p>
              <p>
                Improved foundation model grounding capabilities via action composition and complex spatio-temporal scene understanding.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/cospal.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2412.07072"> -->
                <papertitle>Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding</papertitle>
              </a>
              <br>
              <strong> Akash Kumar, </strong>
              Zsolt Kira, 
              Yogesh Singh Rawat
              <br>
              <em>Under review </em>
              <br>
              <!-- <a href="https://arxiv.org/abs/2412.07072">paper</a> &nbsp/&nbsp -->
              <!-- <a href="https://github.com/AKASH2907/stable-mean-teacher">code</a> -->
              <p></p>
              <p>
                First foundation model adaptation for dense multimodal video detection task without any labels. Context aware and self-paced progressive scene learning approach.
              </p>
            </td>
          </tr>
          
          
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/smt.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.07072">
                <papertitle>Stable Mean Teacher for Semi-Supervised Video Action Detection</papertitle>
              </a>
              <br>
              <strong> Akash Kumar, </strong>
              Sirshapan Mitra, 
              Yogesh Singh Rawat
              <br>
              <em>Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>)</em>, 2025
              <br>
              <a href="https://akash2907.github.io/smt_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/abs/2412.07072">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/stable-mean-teacher">code</a> 
              <p></p>
              <p>
                Learning from mistakes on labelled set and transfer that learning to pseudo labels from unlabeled set to enhance spatio-temporal localization. 
                Class-agnostic spatio-temporal refinement module and temporal coherency constraint for better spatio-temporal localization.
              </p>
            </td>
          </tr>
          
          

          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/semi_active.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2312.07169">
                <papertitle>Semi-supervised Active Learning for Video Action Detection</papertitle>
              </a>
              <br>
              Ayush Singh,
              Aayush J Rana,
              <strong> Akash Kumar, </strong>
              Shruti Vyas,
              Yogesh Singh Rawat
              <br>
              <em>Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>)</em>, 2024
              <br>
              <a href="https://akash2907.github.io/smt_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2312.07169">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/semi-sup-active-learning">code</a> &nbsp/&nbsp
              <a href="images/ssl_active_aaai24.jpg">poster</a>
              <p></p>
              <p>
                High-pass filtering for enhanced pseudo labels to improvise spatio-temporal localization. Simple sample augmentation strategy for informative sample selection.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ssl_teaser.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.06010">
                <papertitle>Benchmarking self-supervised video representation learning </papertitle>
              </a>
              <br>
              <strong> Akash Kumar, </strong>
              Ashlesha Kumar, Vibhav Vineet, Yogesh Singh Rawat
              <br>
              <em> Neural Information Processing (<strong>NeurIPS Workshops</strong>)</em>, 2023 <br>
              <em> 4th Workshop on Self-supervised Learning: Theory and Practices </em>
              <br>
              <a href="https://akash2907.github.io/smt_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2306.06010">paper</a> &nbsp/&nbsp
              <a href="images/sslb_neuripsw23.jpg">poster</a>
              <p></p>
              <p>
                First exhaustive study on impact of pre-training in self-supervised learning for videos. Proposed a simple knowledge distillation 
                approach outperforming previous works with 90% less videos.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/e2essl.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kumar_End-to-End_Semi-Supervised_Learning_for_Video_Action_Detection_CVPR_2022_paper.pdf">
                <papertitle>End-to-End Semi-Supervised Learning for Video Action Detection</papertitle>
              </a>
              <br>
              <strong> Akash Kumar, </strong>
              Yogesh Singh Rawat
              <br>
              <em>Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://akash2907.github.io/smt_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kumar_End-to-End_Semi-Supervised_Learning_for_Video_Action_Detection_CVPR_2022_paper.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/pi-consistency-activity-detection">code</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=kAJV-w6knKA&ab_channel=UCFCRCV">video</a>
              <p></p>
              <p>
                First end-to-end semi-supervised approach for video action detection task. Short-term and long-term smoothness constraints to exploit spatio-temporal coherency.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/cvprw22.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2204.07892">
                <papertitle>Video Action Detection: Analysing Limitations and Challenges</papertitle>
              </a>
              <br>
              Rajat Modi, Aayush Rana,<strong>  Akash Kumar, </strong>
              Praveen Tirupattar, Shruti Vyas, Yogesh Singh Rawat, Mubarak Shah
              <br>
              <em>Computer Vision and Pattern Recognition Conference (<strong>CVPR Workshops</strong>)</em>, 2022 <br>
              <em> 1st Workshop on Vision Datasets Understanding</em>
              <br>
              <a href="https://arxiv.org/abs/2204.07892">paper</a>
              <p></p>
              <p>
                Developed new spatio-temporal surveillance based dataset for real-world challenges.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gabriella.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">
                <papertitle>Gabriella V2: Towards better generalization in surveillance videos for Action Detection</papertitle>
              </a>
              <br>
              Ishan Dave, Zaccheeus Scheffer,<strong>  Akash Kumar, </strong>
              Sania Shiraz, Yogesh Singh Rawat, Mubarak Shah
              <br>
              <em> (<strong>WACV Workshops</strong>)</em>, 2022 <br>
              <em> Human Activity Detection in Multi-Camera Long-Duration Video</em>
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=maFQycAn6Ns&ab_channel=UCFCRCV">video</a>
              <p></p>
              <p>
                Developed new spatio-temporal surveillance based dataset for real-world challenges.
              </p>
            </td>
          </tr>
          
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
                  <tr>
                    <td>
                      <heading>Awards / Recognitions</heading>
                    </td>
                  </tr>
                </tbody></table>
        
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png" width="50%" alt="Trophy Image"></td>
                
                  <td width="75%" valign="center">
            
                <!-- 2nd Prize 2022 -->
                
                <div>
                      <strong>2<sup>nd</sup> place,</strong> 2023 - 
                      IARPA BRIAR: Biometric Recognition and Identification at Altitude and Range
              </div>
              <br>

                <!-- 2nd Prize 2021 -->
                <div>
                    <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv21.slides/tv21.actev.slides.pdf">
                        <strong>2<sup>nd</sup> place,</strong> 2021 - 
                        NIST TRECVID ActEV: Activities in Extended Video
                    </a>
                </div>
                <br>
                <div>
                      <strong>1<sup>st</sup> place,</strong> 2021 - 
                      PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>)
                  </div>
                  <br>
                  <div>
                    <!-- 		        <a href="https://activity-net.org/challenges/2022/challenge.html"> -->
                      Selected for 8th Heidelberg Laureate Forum, Germany, 2021
                            </div>
                            <br>
                <div>
        <!-- 		        <a href="#"> -->
                        <strong>1<sup>st</sup> place,</strong> 2021 - 
                        PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>)
        <!-- 		        </a> -->
                </div>
                <br>
            
                <!-- ORCGS Doctoral Fellowship 2019-2020 -->
                <div>
        <!-- 		        <a href="#"> -->
                        <strong>ORCGS Doctoral Fellowship,</strong> 2020-2021
        <!-- 		        </a> -->
                </div>
                <br>
            
                <!-- Top 0.5% 2013 -->
                <div>
                    <a href="#">
                        <strong>Top 0.01%,</strong> 2015 - 
                        Joint Engineering Entrance-Mains exam, India
                    </a>
                </div>
            </td>
            
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, CVPR 2023, 2024, 2025</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICLR 2023, 2024, 2025</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ECCV 2022, 2024</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICCV 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, NeurIPS 2023, 2024</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICML 2024</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ACMMM 2023, 2024</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, WACV 2023, 2024</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, IEEE Transaction on Image Processing</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, IEEE Transactions on Circuits and Systems for Video Technology</a>
              <br>
            </td>
          </tr>

		
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
